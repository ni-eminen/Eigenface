{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Let's import numpy, PIL and some sklearn modules for the popular olivetti dataset and splitting training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helpers import EigenfaceHelpers, negative_vector\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define constants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SHAPE = (64, 64)\n",
    "DATASET_SIZE = 400"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the dataset, reshape the images into vectors and split it into pieces for training.\n",
    "- We fetch the olivetti dataset via sklearn\n",
    "- Olivetti.images is a collection of vectors, raveled 64x64 sized images\n",
    "- olivetti.target contains the id's of the people in the X array in the corresponding indices\n",
    "- We give the indices to the train_test_split to track which person is in which index after the function shuffles them, this will later be used to determine whether the algorithm predicted the correct person"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Download Olivetti faces dataset\n",
    "olivetti = fetch_olivetti_faces()\n",
    "X = olivetti.images[:DATASET_SIZE]\n",
    "y = olivetti.target[:DATASET_SIZE]\n",
    "\n",
    "# Print info on shapes and reshape where necessary\n",
    "X = X.reshape((DATASET_SIZE, 4096))\n",
    "indices = np.arange(len(X))\n",
    "Xtrain, Xtest, ytrain, ytest, idx_train, idx_test = train_test_split(X, y, indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct the average face from the training set.\n",
    "- Add all training vectors together and divide the sum by the number of images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "training_set = Xtrain\n",
    "# Average face using numpy\n",
    "avg_face = training_set.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Derive normalized faces\n",
    "- Subtract the average face from each of the faces in the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Let's create the matrix A by subtracting the average face from each face in the training set\n",
    "normalized_faces = []\n",
    "neg_avg_face = negative_vector(avg_face)\n",
    "sub = None\n",
    "for v in training_set:\n",
    "    sub = np.subtract(v, avg_face)\n",
    "    normalized_faces.append(sub)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Form the covariance matrix\n",
    "- Transpose the matrix of normalized faces\n",
    "- Multiply the normalized faces matrix with its transposition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Form the covariance matrix\n",
    "normalized_faces_t = np.array(normalized_faces).transpose()\n",
    "\n",
    "cov_matrix = np.cov(np.array(normalized_faces))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "- In order to determine the strongest eigenfaces, we select the eigenvectors with the highest corresponding eigenvalues\n",
    "- Pair the eigenvalues/eigenvectors\n",
    "- Sort the pairs based on the highest eigenvalues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Calculate the eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:, index]) for index in range(len(eigenvalues))]\n",
    "\n",
    "eig_pairs.sort(reverse=True)\n",
    "eigvalues_sort = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sort = [eig_pairs[index][1] for index in range(len(eigenvalues))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select the 20 best eigenvectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Choose the 10 eigenvectors with the highest eigenvalues as the eigenfaces\n",
    "eigenfaces = np.array(eigvectors_sort[:20]).transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create reduced eigenface space and calculate the weights for the projected vectors\n",
    "- Project the eigenfaces to the training_sets transposition by performing a dot product between the two\n",
    "- A weight is calculated by performing a dot product between each normalized face and the projections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "proj_data = np.dot(training_set.transpose(), eigenfaces)\n",
    "proj_data = proj_data.transpose()\n",
    "\n",
    "\n",
    "# Calculate weights for eigenfaces\n",
    "w = np.array([np.dot(proj_data, i) for i in np.array(normalized_faces)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate distance between the weights of each eigenface and the test image\n",
    "- Create the normalized unknown face\n",
    "- Calculate the weights of the normalized unknown weights in respect to the projections\n",
    "- Create the difference vector, which is the weights of the eigenfaces subracted from the weights of the test image\n",
    "- Find the index of the lowest difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.helpers import predict\n",
    "\n",
    "correct_ids = []\n",
    "predicted_ids = []\n",
    "\n",
    "# Store the correct ids and the predicted ids in corresponding indices\n",
    "correct_ids_multi, predicted_ids_multi = predict(Xtest, y, idx_train, idx_test, avg_face, proj_data, w, type=\"\", sample_size=3, threshold=2)\n",
    "\n",
    "correct_ids, predicted_ids = predict(Xtest, y, idx_train, idx_test, avg_face, proj_data, w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40         3\n",
      "           1       1.00      1.00      1.00         4\n",
      "           2       0.50      0.75      0.60         4\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       1.00      1.00      1.00         3\n",
      "           5       1.00      1.00      1.00         3\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      0.75      0.86         4\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         3\n",
      "          11       0.50      0.50      0.50         2\n",
      "          12       0.50      0.67      0.57         3\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       0.67      1.00      0.80         2\n",
      "          15       1.00      0.67      0.80         3\n",
      "          16       1.00      1.00      1.00         4\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         3\n",
      "          22       1.00      0.33      0.50         3\n",
      "          23       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         2\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       1.00      1.00      1.00         3\n",
      "          27       1.00      0.67      0.80         3\n",
      "          28       0.20      1.00      0.33         1\n",
      "          29       1.00      1.00      1.00         1\n",
      "          31       1.00      1.00      1.00         1\n",
      "          32       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         3\n",
      "          34       1.00      1.00      1.00         1\n",
      "          35       0.50      1.00      0.67         1\n",
      "          36       1.00      1.00      1.00         3\n",
      "          37       1.00      1.00      1.00         1\n",
      "          38       1.00      0.43      0.60         7\n",
      "          39       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.84       100\n",
      "   macro avg       0.89      0.88      0.86       100\n",
      "weighted avg       0.90      0.84      0.84       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#print(classification_report(correct_ids, predicted_ids, zero_division=0))\n",
    "print(classification_report(correct_ids_multi, predicted_ids_multi, zero_division=0))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
