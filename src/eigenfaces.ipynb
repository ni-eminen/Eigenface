{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Let's import numpy, PIL and some sklearn modules for the popular olivetti dataset and splitting training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helpers import EigenfaceHelpers, negative_vector\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define constants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SHAPE = (64, 64)\n",
    "DATASET_SIZE = 400"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the dataset, reshape the images into vectors and split it into pieces for training.\n",
    "- We fetch the olivetti dataset via sklearn\n",
    "- Olivetti.images is a collection of vectors, raveled 64x64 sized images\n",
    "- olivetti.target contains the id's of the people in the X array in the corresponding indices\n",
    "- We give the indices to the train_test_split to track which person is in which index after the function shuffles them, this will later be used to determine whether the algorithm predicted the correct person"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7 243  26  22 348 178 279 215 313  89 275 340 356 138 365 236  29 226\n",
      " 357 291 171 242 277 241  97 169 245 266 234 262 292 258 209  36 343 351\n",
      " 394  63 239  38 170  34   5 305 321 197 207 337 274 130 115 270 353 346\n",
      " 393 307 113 125 153 263 186 383 152 214 219 106 345 204 142 325 145 136\n",
      " 211 102 392 150 282 272 288  90  93  74  49  77 259  33 147 109 235  19\n",
      "  75 299 254 363 251 149 247 201  80 137  43 168 399  41 157 112 267 160\n",
      " 161 135 114 335 371 328 359 224 146  73 151 350  96 181 268 228  11  24\n",
      " 379 384 213 273 123 342 352 278  40 310 189 154  50 216  70 134 246 176\n",
      " 332 205 188 103 378   3  44 156 324  62  17  99 217 381 222 284 389 108\n",
      " 320 225 361 252 185 257  59 208 349  79 374  58  12 159 104 329 206 187\n",
      "  92 190 385 192 184 202  78 119 358 280   1 397  91 396 131  86  72  28\n",
      "  31 199 200  21  88  61 175 316 315  27 318 129 306  45 265 107  95 387\n",
      " 180 368 286  64  60 296 250 285 382 139 141 260  10 244  76 303 198 271\n",
      " 163  32 269 238   6  68  67 287  81 386 391 311 173 133 165 290 369 372\n",
      " 124 341  66  65   8 218 314  18 249 289 196 164  20 174 155 166 143  16\n",
      " 118 158 309 105 220 253 111 331  94 375  35   2 281 293  51 339  15  13\n",
      " 193 116 295 121  54 237 110   4 304 248  47 302]\n"
     ]
    }
   ],
   "source": [
    "# Download Olivetti faces dataset\n",
    "olivetti = fetch_olivetti_faces()\n",
    "X = olivetti.images[:DATASET_SIZE]\n",
    "y = olivetti.target[:DATASET_SIZE]\n",
    "\n",
    "# Print info on shapes and reshape where necessary\n",
    "X = X.reshape((DATASET_SIZE, 4096))\n",
    "indices = np.arange(len(X))\n",
    "Xtrain, Xtest, ytrain, ytest, idx_train, idx_test = train_test_split(X, y, indices)\n",
    "print(idx_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct the average face from the training set.\n",
    "- Add all training vectors together and divide the sum by the number of images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "training_set = Xtrain\n",
    "# Average face using numpy\n",
    "avg_face = training_set.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Derive normalized faces\n",
    "- Subtract the average face from each of the faces in the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Let's create the matrix A by subtracting the average face from each face in the training set\n",
    "normalized_faces = []\n",
    "neg_avg_face = negative_vector(avg_face)\n",
    "sub = None\n",
    "for v in training_set:\n",
    "    sub = np.subtract(v, avg_face)\n",
    "    normalized_faces.append(sub)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Form the covariance matrix\n",
    "- Transpose the matrix of normalized faces\n",
    "- Multiply the normalized faces matrix with its transposition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Form the covariance matrix\n",
    "normalized_faces_t = np.array(normalized_faces).transpose()\n",
    "\n",
    "cov_matrix = np.cov(np.array(normalized_faces))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "- In order to determine the strongest eigenfaces, we select the eigenvectors with the highest corresponding eigenvalues\n",
    "- Pair the eigenvalues/eigenvectors\n",
    "- Sort the pairs based on the highest eigenvalues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Calculate the eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:, index]) for index in range(len(eigenvalues))]\n",
    "\n",
    "eig_pairs.sort(reverse=True)\n",
    "eigvalues_sort = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sort = [eig_pairs[index][1] for index in range(len(eigenvalues))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select the 20 best eigenvectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Choose the 10 eigenvectors with the highest eigenvalues as the eigenfaces\n",
    "eigenfaces = np.array(eigvectors_sort[:20]).transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create reduced eigenface space and calculate the weights for the projected vectors\n",
    "- Project the eigenfaces to the training_sets transposition by performing a dot product between the two\n",
    "- A weight is calculated by performing a dot product between each normalized face and the projections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "proj_data = np.dot(training_set.transpose(), eigenfaces)\n",
    "proj_data = proj_data.transpose()\n",
    "\n",
    "# Calculate weights for eigenfaces\n",
    "w = np.array([np.dot(proj_data, i) for i in np.array(normalized_faces)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate distance between the weights of each eigenface and the test image\n",
    "- Create the normalized unknown face\n",
    "- Calculate the weights of the normalized unknown weights in respect to the projections\n",
    "- Create the difference vector, which is the weights of the eigenfaces subracted from the weights of the test image\n",
    "- Find the index of the lowest difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.helpers import predict\n",
    "\n",
    "correct_ids_euclidean = []\n",
    "predicted_ids_euclidean = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from helpers import manhattan_distance, hamming_distance, euclidean_distance\n",
    "# Store the correct ids and the predicted ids in corresponding indices\n",
    "correct_ids = []\n",
    "for i in range(len(Xtest)):\n",
    "    correct_ids.append(y[idx_test[i]])\n",
    "\n",
    "# Make predictions with different distance measurements\n",
    "predicted_ids_multi     = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=manhattan_distance, type=\"KNN\", sample_size=5, threshold=3)\n",
    "predicted_ids_euclidean = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=euclidean_distance)\n",
    "predicted_ids_manhattan = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=manhattan_distance)\n",
    "predicted_ids_hamming   = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=hamming_distance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create classification reports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "multi = classification_report(correct_ids, predicted_ids_multi, zero_division=0, output_dict=True)\n",
    "euclidean = classification_report(correct_ids, predicted_ids_euclidean, zero_division=0, output_dict=True)\n",
    "manhattan = classification_report(correct_ids, predicted_ids_manhattan, zero_division=0, output_dict=True)\n",
    "hamming = classification_report(correct_ids, predicted_ids_hamming, zero_division=0, output_dict=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method               precision      recall         f1-score\n",
      "KNN, (manhattan):    0.74           0.58           0.62\n",
      "euclidean:           0.86           0.84           0.83\n",
      "manhattan:           0.88           0.87           0.86\n",
      "hamming:             0.00           0.02           0.00\n"
     ]
    }
   ],
   "source": [
    "methods = {'KNN, (manhattan)': multi, 'euclidean': euclidean, 'manhattan': manhattan, 'hamming': hamming}\n",
    "\n",
    "print(f'{\"method\":21}{\"precision\":15}{\"recall\":15}{\"f1-score\"}')\n",
    "for key in methods:\n",
    "    precision = methods[key]['weighted avg']['precision'].__str__()[:4]\n",
    "    recall = methods[key]['weighted avg']['recall'].__str__()[:4]\n",
    "    f1 = methods[key]['weighted avg']['f1-score'].__str__()[:4]\n",
    "    print(f\"{key + ':':20} {precision:15}{recall:15}{f1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
