{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Let's import numpy, PIL and some sklearn modules for the popular olivetti dataset and splitting training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helpers import EigenfaceHelpers, negative_vector\n",
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define constants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SHAPE = (64, 64)\n",
    "DATASET_SIZE = 400"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the dataset, reshape the images into vectors and split it into pieces for training.\n",
    "- We fetch the olivetti dataset via sklearn\n",
    "- Olivetti.images is a collection of vectors, raveled 64x64 sized images\n",
    "- olivetti.target contains the id's of the people in the X array in the corresponding indices\n",
    "- We give the indices to the train_test_split to track which person is in which index after the function shuffles them, this will later be used to determine whether the algorithm predicted the correct person"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Download Olivetti faces dataset\n",
    "olivetti = fetch_olivetti_faces()\n",
    "X = olivetti.images[:DATASET_SIZE]\n",
    "y = olivetti.target[:DATASET_SIZE]\n",
    "\n",
    "# Print info on shapes and reshape where necessary\n",
    "X = X.reshape((DATASET_SIZE, 4096))\n",
    "indices = np.arange(len(X))\n",
    "Xtrain, Xtest, ytrain, ytest, idx_train, idx_test = train_test_split(X, y, indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct the average face from the training set.\n",
    "- Add all training vectors together and divide the sum by the number of images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "training_set = Xtrain\n",
    "# Average face using numpy\n",
    "avg_face = training_set.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Derive normalized faces\n",
    "- Subtract the average face from each of the faces in the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Let's create the matrix A by subtracting the average face from each face in the training set\n",
    "normalized_faces = []\n",
    "neg_avg_face = negative_vector(avg_face)\n",
    "sub = None\n",
    "for v in training_set:\n",
    "    sub = np.subtract(v, avg_face)\n",
    "    normalized_faces.append(sub)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Form the covariance matrix\n",
    "- Transpose the matrix of normalized faces\n",
    "- Multiply the normalized faces matrix with its transposition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Form the covariance matrix\n",
    "normalized_faces_t = np.array(normalized_faces).transpose()\n",
    "\n",
    "cov_matrix = np.cov(np.array(normalized_faces))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the eigenvalues and eigenvectors for the covariance matrix\n",
    "- In order to determine the strongest eigenfaces, we select the eigenvectors with the highest corresponding eigenvalues\n",
    "- Pair the eigenvalues/eigenvectors\n",
    "- Sort the pairs based on the highest eigenvalues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Calculate the eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "eig_pairs = [(eigenvalues[index], eigenvectors[:, index]) for index in range(len(eigenvalues))]\n",
    "\n",
    "eig_pairs.sort(reverse=True)\n",
    "eigvalues_sort = [eig_pairs[index][0] for index in range(len(eigenvalues))]\n",
    "eigvectors_sort = [eig_pairs[index][1] for index in range(len(eigenvalues))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select the 20 best eigenvectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Choose the 10 eigenvectors with the highest eigenvalues as the eigenfaces\n",
    "eigenfaces = np.array(eigvectors_sort[:20]).transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create reduced eigenface space and calculate the weights for the projected vectors\n",
    "- Project the eigenfaces to the training_sets transposition by performing a dot product between the two\n",
    "- A weight is calculated by performing a dot product between each normalized face and the projections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "proj_data = np.dot(training_set.transpose(), eigenfaces)\n",
    "proj_data = proj_data.transpose()\n",
    "\n",
    "# Calculate weights for eigenfaces\n",
    "w = np.array([np.dot(proj_data, i) for i in np.array(normalized_faces)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate distance between the weights of each eigenface and the test image\n",
    "- Create the normalized unknown face\n",
    "- Calculate the weights of the normalized unknown weights in respect to the projections\n",
    "- Create the difference vector, which is the weights of the eigenfaces subracted from the weights of the test image\n",
    "- Find the index of the lowest difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.helpers import predict\n",
    "\n",
    "correct_ids_euclidean = []\n",
    "predicted_ids_euclidean = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from helpers import manhattan_distance, hamming_distance, euclidean_distance\n",
    "# Store the correct ids and the predicted ids in corresponding indices\n",
    "correct_ids = []\n",
    "for i in range(len(Xtest)):\n",
    "    correct_ids.append(y[idx_test[i]])\n",
    "\n",
    "# Make predictions with different distance measurements\n",
    "predicted_ids_multi     = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=manhattan_distance, type=\"KNN\", sample_size=5, threshold=3)\n",
    "predicted_ids_euclidean = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=euclidean_distance)\n",
    "predicted_ids_manhattan = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=manhattan_distance)\n",
    "predicted_ids_hamming   = predict(Xtest, y, idx_train, avg_face, proj_data, w, distance_func=hamming_distance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create classification reports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "multi = classification_report(correct_ids, predicted_ids_multi, zero_division=0, output_dict=True)\n",
    "euclidean = classification_report(correct_ids, predicted_ids_euclidean, zero_division=0, output_dict=True)\n",
    "manhattan = classification_report(correct_ids, predicted_ids_manhattan, zero_division=0, output_dict=True)\n",
    "hamming = classification_report(correct_ids, predicted_ids_hamming, zero_division=0, output_dict=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compare performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method               precision      recall         f1-score\n",
      "KNN, (manhattan):    0.60           0.41           0.44\n",
      "euclidean:           0.88           0.81           0.82\n",
      "manhattan:           0.89           0.86           0.86\n",
      "hamming:             0.0            0.0            0.0\n"
     ]
    }
   ],
   "source": [
    "methods = {'KNN, (manhattan)': multi, 'euclidean': euclidean, 'manhattan': manhattan, 'hamming': hamming}\n",
    "\n",
    "print(f'{\"method\":21}{\"precision\":15}{\"recall\":15}{\"f1-score\"}')\n",
    "for key in methods:\n",
    "    precision = methods[key]['weighted avg']['precision'].__str__()[:4]\n",
    "    recall = methods[key]['weighted avg']['recall'].__str__()[:4]\n",
    "    f1 = methods[key]['weighted avg']['f1-score'].__str__()[:4]\n",
    "    print(f\"{key + ':':20} {precision:15}{recall:15}{f1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
